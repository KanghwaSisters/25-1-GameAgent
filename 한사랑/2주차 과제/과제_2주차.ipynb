{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vECysj7UNoM"
      },
      "source": [
        "### 딥러닝 실습 과제 2주차 - CNN을 활용하여 보드 전체를 입력받아 9칸 상태 예측\n",
        "\n",
        "다음  세 가지 활동을 해봅시다.\n",
        "\n",
        "01. **모델 설계**: CNN 기반 분류 모델 활용\n",
        "02. **손실 함수 정의**: 각 칸에 대해 3-클래스 분류\n",
        "03. **학습 및 평가**: 시각화 함수, 정확도 측정, confusion matrix 등"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3_fdD7Y2iY4"
      },
      "source": [
        "TTTDataset.zip을 불러와 문제에서 요하는 코드를 구현하세요.\n",
        "\n",
        "💡 **데이터 구조**  \n",
        "- **`image_black`** : 이미지 데이터  \n",
        "- **`labels`** : 타겟 데이터  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vscode 작업을 위한 필수 패키지 설치\n",
        "%pip install torch torchvision numpy matplotlib scikit-learn tqdm pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BhRH2q-fV8lc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCpJfLtnagvV"
      },
      "source": [
        "#### 00. 클래스\n",
        "정의한 클래스를 이용해 실행해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ymwd8gfCYHiq"
      },
      "outputs": [],
      "source": [
        "class TTTDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, transform=None):\n",
        "        \"\"\"\n",
        "        틱택토 데이터셋을 PyTorch Dataset 형태로 변환.\n",
        "        :param image_paths: 이미지 파일 경로 리스트\n",
        "        :param label_paths: 레이블 JSON 파일 경로 리스트\n",
        "        :param transform: 이미지 전처리 변환\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.transform = transform\n",
        "        self.data = self._load_data()\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\" 이미지 & 레이블 로드 \"\"\"\n",
        "        data = []\n",
        "        for img_path, lbl_path in zip(self.image_paths, self.label_paths):\n",
        "            # 이미지를 흑백(Grayscale)로 변환\n",
        "            image = Image.open(img_path).convert(\"L\")  # \"RGB\" 대신 \"L\" 사용\n",
        "\n",
        "            # JSON 레이블 로드\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                labels = json.load(f)\n",
        "\n",
        "            # 레이블을 숫자로 변환 (O=1, X=-1, blank=0)\n",
        "            label_tensor = torch.tensor(\n",
        "                [1 if v == \"O\" else -1 if v == \"X\" else 0 for v in labels.values()],\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "            data.append((image, label_tensor))\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" 데이터셋 크기 반환 \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" 데이터셋에서 idx 번째 샘플(이미지 & 레이블)을 가져오는 역할 \"\"\"\n",
        "        image, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이미지 파일 수: 453\n",
            "라벨 파일 수: 453\n",
            "첫 번째 이미지 경로: ../TTTDataset/image_black/01.jpg\n",
            "첫 번째 레이블 경로: ../TTTDataset/labels/01_labels.json\n",
            "\n",
            "원본 이미지 크기: (512, 512)\n",
            "전처리 후 텐서 크기: torch.Size([1, 224, 224])\n",
            "텐서 값 범위: -1.0000 ~ 1.0000\n"
          ]
        }
      ],
      "source": [
        "# 01. 이미지와 레이블 파일 경로 로드\n",
        "image_dir = os.path.join(\"../TTTDataset\", \"image_black\")\n",
        "labels_dir = os.path.join(\"../TTTDataset\", \"labels\")\n",
        "\n",
        "# 모든 이미지와 라벨 파일의 경로 가져오기 (jpg, JPG 확장자 모두)\n",
        "jpg_image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
        "JPG_image_paths = glob.glob(os.path.join(image_dir, \"*.JPG\"))\n",
        "image_paths = sorted(jpg_image_paths + JPG_image_paths)\n",
        "label_paths = sorted(glob.glob(os.path.join(labels_dir, \"*.json\")))\n",
        "\n",
        "print(f\"이미지 파일 수: {len(image_paths)}\")\n",
        "print(f\"라벨 파일 수: {len(label_paths)}\")\n",
        "print(f\"첫 번째 이미지 경로: {image_paths[0]}\")\n",
        "print(f\"첫 번째 레이블 경로: {label_paths[0]}\")\n",
        "print()\n",
        "\n",
        "# 02. 이미지 전처리를 위한 transform 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),               # 이미지 크기 조정\n",
        "    transforms.ToTensor(),                       # PIL 이미지를 텐서로 변환 (0-255 → 0-1)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # 흑백 이미지 정규화 (-1 ~ 1)\n",
        "])\n",
        "\n",
        "# 전처리가 잘 되는지 확인하기 - 첫 번째 이미지에 적용\n",
        "sample_image = Image.open(image_paths[0]).convert(\"L\")\n",
        "processed_image = transform(sample_image)\n",
        "print(f\"원본 이미지 크기: {sample_image.size}\")\n",
        "print(f\"전처리 후 텐서 크기: {processed_image.shape}\")\n",
        "print(f\"텐서 값 범위: {processed_image.min():.4f} ~ {processed_image.max():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 데이터셋 크기: 453\n",
            "학습 데이터셋 크기: 317\n",
            "검증 데이터셋 크기: 67\n",
            "테스트 데이터셋 크기: 69\n"
          ]
        }
      ],
      "source": [
        "# 03. 데이터셋 생성\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# 전체 데이터셋 생성\n",
        "full_dataset = TTTDataset(image_paths, label_paths, transform=transform)\n",
        "\n",
        "# 데이터 분할 비율 설정\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# 전체 데이터 개수\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(total_size * train_ratio)\n",
        "val_size = int(total_size * val_ratio)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# 데이터셋 분할하기\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset, \n",
        "    [train_size, val_size, test_size]\n",
        ")\n",
        "\n",
        "# 분할된 데이터셋 정보 출력\n",
        "print(f\"전체 데이터셋 크기: {total_size}\")\n",
        "print(f\"학습 데이터셋 크기: {len(train_dataset)}\")\n",
        "print(f\"검증 데이터셋 크기: {len(val_dataset)}\")\n",
        "print(f\"테스트 데이터셋 크기: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터 배치 수: 10\n",
            "검증 데이터 배치 수: 3\n",
            "테스트 데이터 배치 수: 3\n"
          ]
        }
      ],
      "source": [
        "# 04. 데이터로더 생성\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 배치 크기 설정\n",
        "batch_size = 32\n",
        "\n",
        "# 전체 데이터로더 생성\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,  # 학습 데이터는 섞어서 사용\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,  # 검증 데이터는 섞지 않음\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,  # 테스트 데이터는 섞지 않음\n",
        ")\n",
        "\n",
        "# DataLoader 정보 출력\n",
        "print(f\"학습 데이터 배치 수: {len(train_loader)}\")\n",
        "print(f\"검증 데이터 배치 수: {len(val_loader)}\")\n",
        "print(f\"테스트 데이터 배치 수: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQgBNd9aZNk"
      },
      "source": [
        "#### 01. 모델 설계: CNN 기반 분류 모델 활용\n",
        "- 입력: 틱택토 보드 전체 이미지 (예: 128x128)\n",
        "- 출력: 9개의 상태(각 칸마다 O, X, blank 중 하나)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ue-8ZaunXd1q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "ImprovedTTTBoardCNN(\n",
            "  (initial_conv): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (spp): SpatialPyramidPooling()\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=10752, out_features=1024, bias=True)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Dropout(p=0.3, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=27, bias=True)\n",
            "  )\n",
            "  (attention): SelfAttention(\n",
            "    (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedTTTBoardCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedTTTBoardCNN, self).__init__()\n",
        "        \n",
        "        # ResNet 스타일 구조와 더 깊은 네트워크\n",
        "        # 입력: 1채널 224x224 그레이스케일 이미지\n",
        "        \n",
        "        # 초기 컨볼루션 레이어\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )  # 출력: 64 x 56 x 56\n",
        "        \n",
        "        # ResNet 스타일 블록\n",
        "        self.block1 = self._make_res_block(64, 128)  # 출력: 128 x 28 x 28\n",
        "        self.block2 = self._make_res_block(128, 256)  # 출력: 256 x 14 x 14\n",
        "        self.block3 = self._make_res_block(256, 512)  # 출력: 512 x 7 x 7\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # 출력: 512 x 1 x 1\n",
        "        \n",
        "        # 공간 피라미드 풀링 (여러 스케일에서 특징 추출)\n",
        "        self.spp = SpatialPyramidPooling([1, 2, 4])\n",
        "        \n",
        "        # 최종 분류 레이어\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * (1 + 4 + 16), 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 9 * 3)  # 9개 셀, 각 셀마다 3가지 클래스\n",
        "        )\n",
        "        \n",
        "        # 데이터 증강을 위한 자가주의(self-attention) 모듈\n",
        "        self.attention = SelfAttention(512)\n",
        "        \n",
        "        # 가중치 초기화\n",
        "        self._initialize_weights()\n",
        "        \n",
        "    def _make_res_block(self, in_channels, out_channels):\n",
        "        \"\"\"ResNet 스타일 블록 생성\"\"\"\n",
        "        return nn.Sequential(\n",
        "            # 첫 번째 컨볼루션 유닛\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # 두 번째 컨볼루션 유닛\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # 세 번째 컨볼루션 유닛 (잔차 연결)\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"네트워크 가중치 초기화\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # 초기 특징 추출\n",
        "        x = self.initial_conv(x)\n",
        "        \n",
        "        # ResNet 스타일 블록 통과\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        \n",
        "        # 자가주의 적용\n",
        "        x = self.attention(x)\n",
        "        \n",
        "        # 공간 피라미드 풀링\n",
        "        x = self.spp(x)\n",
        "        \n",
        "        # 분류\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        # 출력을 9개 셀, 각 셀마다 3 클래스로 재구성\n",
        "        return x.view(-1, 9, 3)\n",
        "\n",
        "\n",
        "class SpatialPyramidPooling(nn.Module):\n",
        "    \"\"\"공간 피라미드 풀링 - 다양한 크기의 특징을 감지하는 데 도움\"\"\"\n",
        "    def __init__(self, levels):\n",
        "        super(SpatialPyramidPooling, self).__init__()\n",
        "        self.levels = levels\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, channels, h, w = x.size()\n",
        "        features = []\n",
        "        \n",
        "        for level in self.levels:\n",
        "            # 각 레벨에서 pooling 수행\n",
        "            pool = nn.AdaptiveMaxPool2d((level, level))\n",
        "            pooled = pool(x).view(batch_size, channels, -1)\n",
        "            features.append(pooled)\n",
        "        \n",
        "        # 모든 레벨 특징 연결\n",
        "        x = torch.cat(features, dim=2)\n",
        "        return x.view(batch_size, -1)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"자가주의 모듈 - 이미지의 중요한 부분에 집중\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        \n",
        "        # 쿼리 생성\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, width*height).permute(0, 2, 1)\n",
        "        \n",
        "        # 키 생성\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, width*height)\n",
        "        \n",
        "        # 어텐션 맵 계산\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        # 값 생성\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, width*height)\n",
        "        \n",
        "        # 어텐션 적용\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "        \n",
        "        # 잔차 연결\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "\n",
        "# 모델 초기화 및 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedTTTBoardCNN().to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYVGt8R27tXu"
      },
      "source": [
        "#### 02. 손실 함수 정의: 각 칸에 대해 3-클래스 분류\n",
        "- 다중 클래스 분류이므로 CrossEntropyLoss 사용 가능\n",
        "- 9개 칸을 각각 분류하는 방식으로 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8KvefRQ4XhtC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def ttt_loss_function(predictions, targets):\n",
        "    \n",
        "    # CrossEntropyLoss 정의\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # 레이블 변환: [-1(X), 0(blank), 1(O)] → [2, 0, 1]\n",
        "    converted_targets = torch.zeros_like(targets, dtype=torch.long)\n",
        "    converted_targets[targets == -1] = 2  # X → 2\n",
        "    converted_targets[targets == 0] = 0   # blank → 0\n",
        "    converted_targets[targets == 1] = 1   # O → 1\n",
        "    \n",
        "    # 9개 셀 각각에 대한 손실 계산\n",
        "    total_loss = 0\n",
        "    for i in range(9):\n",
        "        cell_pred = predictions[:, i, :]       # i번째 셀 예측값\n",
        "        cell_target = converted_targets[:, i]  # i번째 셀 실제값\n",
        "        total_loss += loss_fn(cell_pred, cell_target)\n",
        "    \n",
        "    # 평균 손실 반환\n",
        "    return total_loss / 9\n",
        "\n",
        "# 최적화 함수 설정 - Adam\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugSpuxB27va_"
      },
      "source": [
        "#### 03. 학습 및 평가: 시각화 함수, 정확도 측정, confusion matrix 등\n",
        "- 학습 데이터로 모델을 훈련하고, 정확도 측정\n",
        "- 예측이 잘 되는지 시각화하여 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10]\n",
            "Train - Loss: 1.0883, Accuracy: 0.00%\n",
            "Valid - Loss: 1.0947, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [2/10]\n",
            "Train - Loss: 1.0491, Accuracy: 0.32%\n",
            "Valid - Loss: 1.1553, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [3/10]\n",
            "Train - Loss: 0.9636, Accuracy: 0.95%\n",
            "Valid - Loss: 1.5179, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [4/10]\n",
            "Train - Loss: 0.8803, Accuracy: 2.21%\n",
            "Valid - Loss: 1.9323, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [5/10]\n",
            "Train - Loss: 0.8020, Accuracy: 2.21%\n",
            "Valid - Loss: 1.7228, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [6/10]\n",
            "Train - Loss: 0.6962, Accuracy: 7.26%\n",
            "Valid - Loss: 1.1918, Accuracy: 7.46%\n",
            "--------------------------------------------------\n",
            "Epoch [7/10]\n",
            "Train - Loss: 0.6182, Accuracy: 11.36%\n",
            "Valid - Loss: 0.9554, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [8/10]\n",
            "Train - Loss: 0.5706, Accuracy: 12.30%\n",
            "Valid - Loss: 0.6089, Accuracy: 13.43%\n",
            "--------------------------------------------------\n",
            "Epoch [9/10]\n",
            "Train - Loss: 0.5227, Accuracy: 17.67%\n",
            "Valid - Loss: 0.5894, Accuracy: 14.93%\n",
            "--------------------------------------------------\n",
            "Epoch [10/10]\n",
            "Train - Loss: 0.4824, Accuracy: 23.97%\n",
            "Valid - Loss: 0.6120, Accuracy: 11.94%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        # 훈련 단계\n",
        "        train_loss, train_acc = evaluate(model, train_loader, criterion, optimizer, is_training=True)\n",
        "        \n",
        "        # 검증 단계\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, is_training=False)\n",
        "        \n",
        "        # 현재 epoch 결과 출력\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2%}\")\n",
        "        print(f\"Valid - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2%}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, criterion, optimizer=None, is_training=True):\n",
        "    model.train() if is_training else model.eval()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.set_grad_enabled(is_training):\n",
        "        for images, targets in data_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            # 학습 모드일 경우 그래디언트 초기화\n",
        "            if is_training and optimizer:\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "            # 순전파\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # 학습 모드일 경우 역전파 및 최적화\n",
        "            if is_training and optimizer:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # 예측 및 정확도 계산\n",
        "            pred_classes = torch.argmax(outputs, dim=2)\n",
        "            converted_targets = targets.clone()\n",
        "            converted_targets[targets == -1] = 2  # X를 2로 변환\n",
        "            \n",
        "            correct += (pred_classes == converted_targets).all(dim=1).sum().item()\n",
        "            total += targets.size(0)\n",
        "    \n",
        "    return running_loss / len(data_loader), correct / total\n",
        "\n",
        "\n",
        "# 모델 학습 실행\n",
        "num_epochs = 10\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader, \n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=ttt_loss_function,\n",
        "    num_epochs=num_epochs\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
