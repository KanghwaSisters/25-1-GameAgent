{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vECysj7UNoM"
      },
      "source": [
        "### ë”¥ëŸ¬ë‹ ì‹¤ìŠµ ê³¼ì œ 2ì£¼ì°¨ - CNNì„ í™œìš©í•˜ì—¬ ë³´ë“œ ì „ì²´ë¥¼ ì…ë ¥ë°›ì•„ 9ì¹¸ ìƒíƒœ ì˜ˆì¸¡\n",
        "\n",
        "ë‹¤ìŒ  ì„¸ ê°€ì§€ í™œë™ì„ í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "01. **ëª¨ë¸ ì„¤ê³„**: CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ í™œìš©\n",
        "02. **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜**: ê° ì¹¸ì— ëŒ€í•´ 3-í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "03. **í•™ìŠµ ë° í‰ê°€**: ì‹œê°í™” í•¨ìˆ˜, ì •í™•ë„ ì¸¡ì •, confusion matrix ë“±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3_fdD7Y2iY4"
      },
      "source": [
        "TTTDataset.zipì„ ë¶ˆëŸ¬ì™€ ë¬¸ì œì—ì„œ ìš”í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "\n",
        "ğŸ’¡ **ë°ì´í„° êµ¬ì¡°**  \n",
        "- **`image_black`** : ì´ë¯¸ì§€ ë°ì´í„°  \n",
        "- **`labels`** : íƒ€ê²Ÿ ë°ì´í„°  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vscode ì‘ì—…ì„ ìœ„í•œ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "%pip install torch torchvision numpy matplotlib scikit-learn tqdm pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BhRH2q-fV8lc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCpJfLtnagvV"
      },
      "source": [
        "#### 00. í´ë˜ìŠ¤\n",
        "ì •ì˜í•œ í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ymwd8gfCYHiq"
      },
      "outputs": [],
      "source": [
        "class TTTDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, transform=None):\n",
        "        \"\"\"\n",
        "        í‹±íƒí†  ë°ì´í„°ì…‹ì„ PyTorch Dataset í˜•íƒœë¡œ ë³€í™˜.\n",
        "        :param image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "        :param label_paths: ë ˆì´ë¸” JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "        :param transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.transform = transform\n",
        "        self.data = self._load_data()\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\" ì´ë¯¸ì§€ & ë ˆì´ë¸” ë¡œë“œ \"\"\"\n",
        "        data = []\n",
        "        for img_path, lbl_path in zip(self.image_paths, self.label_paths):\n",
        "            # ì´ë¯¸ì§€ë¥¼ í‘ë°±(Grayscale)ë¡œ ë³€í™˜\n",
        "            image = Image.open(img_path).convert(\"L\")  # \"RGB\" ëŒ€ì‹  \"L\" ì‚¬ìš©\n",
        "\n",
        "            # JSON ë ˆì´ë¸” ë¡œë“œ\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                labels = json.load(f)\n",
        "\n",
        "            # ë ˆì´ë¸”ì„ ìˆ«ìë¡œ ë³€í™˜ (O=1, X=-1, blank=0)\n",
        "            label_tensor = torch.tensor(\n",
        "                [1 if v == \"O\" else -1 if v == \"X\" else 0 for v in labels.values()],\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "            data.append((image, label_tensor))\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜ \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" ë°ì´í„°ì…‹ì—ì„œ idx ë²ˆì§¸ ìƒ˜í”Œ(ì´ë¯¸ì§€ & ë ˆì´ë¸”)ì„ ê°€ì ¸ì˜¤ëŠ” ì—­í•  \"\"\"\n",
        "        image, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: 453\n",
            "ë¼ë²¨ íŒŒì¼ ìˆ˜: 453\n",
            "ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê²½ë¡œ: ../TTTDataset/image_black/01.jpg\n",
            "ì²« ë²ˆì§¸ ë ˆì´ë¸” ê²½ë¡œ: ../TTTDataset/labels/01_labels.json\n",
            "\n",
            "ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: (512, 512)\n",
            "ì „ì²˜ë¦¬ í›„ í…ì„œ í¬ê¸°: torch.Size([1, 224, 224])\n",
            "í…ì„œ ê°’ ë²”ìœ„: -1.0000 ~ 1.0000\n"
          ]
        }
      ],
      "source": [
        "# 01. ì´ë¯¸ì§€ì™€ ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œ ë¡œë“œ\n",
        "image_dir = os.path.join(\"../TTTDataset\", \"image_black\")\n",
        "labels_dir = os.path.join(\"../TTTDataset\", \"labels\")\n",
        "\n",
        "# ëª¨ë“  ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ì˜ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (jpg, JPG í™•ì¥ì ëª¨ë‘)\n",
        "jpg_image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
        "JPG_image_paths = glob.glob(os.path.join(image_dir, \"*.JPG\"))\n",
        "image_paths = sorted(jpg_image_paths + JPG_image_paths)\n",
        "label_paths = sorted(glob.glob(os.path.join(labels_dir, \"*.json\")))\n",
        "\n",
        "print(f\"ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_paths)}\")\n",
        "print(f\"ë¼ë²¨ íŒŒì¼ ìˆ˜: {len(label_paths)}\")\n",
        "print(f\"ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê²½ë¡œ: {image_paths[0]}\")\n",
        "print(f\"ì²« ë²ˆì§¸ ë ˆì´ë¸” ê²½ë¡œ: {label_paths[0]}\")\n",
        "print()\n",
        "\n",
        "# 02. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ transform ì •ì˜\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),               # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n",
        "    transforms.ToTensor(),                       # PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜ (0-255 â†’ 0-1)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # í‘ë°± ì´ë¯¸ì§€ ì •ê·œí™” (-1 ~ 1)\n",
        "])\n",
        "\n",
        "# ì „ì²˜ë¦¬ê°€ ì˜ ë˜ëŠ”ì§€ í™•ì¸í•˜ê¸° - ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì— ì ìš©\n",
        "sample_image = Image.open(image_paths[0]).convert(\"L\")\n",
        "processed_image = transform(sample_image)\n",
        "print(f\"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {sample_image.size}\")\n",
        "print(f\"ì „ì²˜ë¦¬ í›„ í…ì„œ í¬ê¸°: {processed_image.shape}\")\n",
        "print(f\"í…ì„œ ê°’ ë²”ìœ„: {processed_image.min():.4f} ~ {processed_image.max():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: 453\n",
            "í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: 317\n",
            "ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: 67\n",
            "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸°: 69\n"
          ]
        }
      ],
      "source": [
        "# 03. ë°ì´í„°ì…‹ ìƒì„±\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±\n",
        "full_dataset = TTTDataset(image_paths, label_paths, transform=transform)\n",
        "\n",
        "# ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì„¤ì •\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# ì „ì²´ ë°ì´í„° ê°œìˆ˜\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(total_size * train_ratio)\n",
        "val_size = int(total_size * val_ratio)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¶„í• í•˜ê¸°\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset, \n",
        "    [train_size, val_size, test_size]\n",
        ")\n",
        "\n",
        "# ë¶„í• ëœ ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥\n",
        "print(f\"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {total_size}\")\n",
        "print(f\"í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(val_dataset)}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í•™ìŠµ ë°ì´í„° ë°°ì¹˜ ìˆ˜: 10\n",
            "ê²€ì¦ ë°ì´í„° ë°°ì¹˜ ìˆ˜: 3\n",
            "í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°°ì¹˜ ìˆ˜: 3\n"
          ]
        }
      ],
      "source": [
        "# 04. ë°ì´í„°ë¡œë” ìƒì„±\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
        "batch_size = 32\n",
        "\n",
        "# ì „ì²´ ë°ì´í„°ë¡œë” ìƒì„±\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,  # í•™ìŠµ ë°ì´í„°ëŠ” ì„ì–´ì„œ ì‚¬ìš©\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,  # ê²€ì¦ ë°ì´í„°ëŠ” ì„ì§€ ì•ŠìŒ\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì„ì§€ ì•ŠìŒ\n",
        ")\n",
        "\n",
        "# DataLoader ì •ë³´ ì¶œë ¥\n",
        "print(f\"í•™ìŠµ ë°ì´í„° ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„° ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°°ì¹˜ ìˆ˜: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQgBNd9aZNk"
      },
      "source": [
        "#### 01. ëª¨ë¸ ì„¤ê³„: CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ í™œìš©\n",
        "- ì…ë ¥: í‹±íƒí†  ë³´ë“œ ì „ì²´ ì´ë¯¸ì§€ (ì˜ˆ: 128x128)\n",
        "- ì¶œë ¥: 9ê°œì˜ ìƒíƒœ(ê° ì¹¸ë§ˆë‹¤ O, X, blank ì¤‘ í•˜ë‚˜)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ue-8ZaunXd1q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "ImprovedTTTBoardCNN(\n",
            "  (initial_conv): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (block2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (block3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (spp): SpatialPyramidPooling()\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=10752, out_features=1024, bias=True)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Dropout(p=0.3, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=27, bias=True)\n",
            "  )\n",
            "  (attention): SelfAttention(\n",
            "    (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedTTTBoardCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedTTTBoardCNN, self).__init__()\n",
        "        \n",
        "        # ResNet ìŠ¤íƒ€ì¼ êµ¬ì¡°ì™€ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬\n",
        "        # ì…ë ¥: 1ì±„ë„ 224x224 ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€\n",
        "        \n",
        "        # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )  # ì¶œë ¥: 64 x 56 x 56\n",
        "        \n",
        "        # ResNet ìŠ¤íƒ€ì¼ ë¸”ë¡\n",
        "        self.block1 = self._make_res_block(64, 128)  # ì¶œë ¥: 128 x 28 x 28\n",
        "        self.block2 = self._make_res_block(128, 256)  # ì¶œë ¥: 256 x 14 x 14\n",
        "        self.block3 = self._make_res_block(256, 512)  # ì¶œë ¥: 512 x 7 x 7\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # ì¶œë ¥: 512 x 1 x 1\n",
        "        \n",
        "        # ê³µê°„ í”¼ë¼ë¯¸ë“œ í’€ë§ (ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ì—ì„œ íŠ¹ì§• ì¶”ì¶œ)\n",
        "        self.spp = SpatialPyramidPooling([1, 2, 4])\n",
        "        \n",
        "        # ìµœì¢… ë¶„ë¥˜ ë ˆì´ì–´\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * (1 + 4 + 16), 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 9 * 3)  # 9ê°œ ì…€, ê° ì…€ë§ˆë‹¤ 3ê°€ì§€ í´ë˜ìŠ¤\n",
        "        )\n",
        "        \n",
        "        # ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ìê°€ì£¼ì˜(self-attention) ëª¨ë“ˆ\n",
        "        self.attention = SelfAttention(512)\n",
        "        \n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "        self._initialize_weights()\n",
        "        \n",
        "    def _make_res_block(self, in_channels, out_channels):\n",
        "        \"\"\"ResNet ìŠ¤íƒ€ì¼ ë¸”ë¡ ìƒì„±\"\"\"\n",
        "        return nn.Sequential(\n",
        "            # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ìœ ë‹›\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ìœ ë‹›\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # ì„¸ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ìœ ë‹› (ì”ì°¨ ì—°ê²°)\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ\n",
        "        x = self.initial_conv(x)\n",
        "        \n",
        "        # ResNet ìŠ¤íƒ€ì¼ ë¸”ë¡ í†µê³¼\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        \n",
        "        # ìê°€ì£¼ì˜ ì ìš©\n",
        "        x = self.attention(x)\n",
        "        \n",
        "        # ê³µê°„ í”¼ë¼ë¯¸ë“œ í’€ë§\n",
        "        x = self.spp(x)\n",
        "        \n",
        "        # ë¶„ë¥˜\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        # ì¶œë ¥ì„ 9ê°œ ì…€, ê° ì…€ë§ˆë‹¤ 3 í´ë˜ìŠ¤ë¡œ ì¬êµ¬ì„±\n",
        "        return x.view(-1, 9, 3)\n",
        "\n",
        "\n",
        "class SpatialPyramidPooling(nn.Module):\n",
        "    \"\"\"ê³µê°„ í”¼ë¼ë¯¸ë“œ í’€ë§ - ë‹¤ì–‘í•œ í¬ê¸°ì˜ íŠ¹ì§•ì„ ê°ì§€í•˜ëŠ” ë° ë„ì›€\"\"\"\n",
        "    def __init__(self, levels):\n",
        "        super(SpatialPyramidPooling, self).__init__()\n",
        "        self.levels = levels\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, channels, h, w = x.size()\n",
        "        features = []\n",
        "        \n",
        "        for level in self.levels:\n",
        "            # ê° ë ˆë²¨ì—ì„œ pooling ìˆ˜í–‰\n",
        "            pool = nn.AdaptiveMaxPool2d((level, level))\n",
        "            pooled = pool(x).view(batch_size, channels, -1)\n",
        "            features.append(pooled)\n",
        "        \n",
        "        # ëª¨ë“  ë ˆë²¨ íŠ¹ì§• ì—°ê²°\n",
        "        x = torch.cat(features, dim=2)\n",
        "        return x.view(batch_size, -1)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"ìê°€ì£¼ì˜ ëª¨ë“ˆ - ì´ë¯¸ì§€ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels//8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        \n",
        "        # ì¿¼ë¦¬ ìƒì„±\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, width*height).permute(0, 2, 1)\n",
        "        \n",
        "        # í‚¤ ìƒì„±\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, width*height)\n",
        "        \n",
        "        # ì–´í…ì…˜ ë§µ ê³„ì‚°\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        # ê°’ ìƒì„±\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, width*height)\n",
        "        \n",
        "        # ì–´í…ì…˜ ì ìš©\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "        \n",
        "        # ì”ì°¨ ì—°ê²°\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™” ë° ì¥ì¹˜ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedTTTBoardCNN().to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYVGt8R27tXu"
      },
      "source": [
        "#### 02. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜: ê° ì¹¸ì— ëŒ€í•´ 3-í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì´ë¯€ë¡œ CrossEntropyLoss ì‚¬ìš© ê°€ëŠ¥\n",
        "- 9ê°œ ì¹¸ì„ ê°ê° ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ êµ¬ì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8KvefRQ4XhtC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def ttt_loss_function(predictions, targets):\n",
        "    \n",
        "    # CrossEntropyLoss ì •ì˜\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # ë ˆì´ë¸” ë³€í™˜: [-1(X), 0(blank), 1(O)] â†’ [2, 0, 1]\n",
        "    converted_targets = torch.zeros_like(targets, dtype=torch.long)\n",
        "    converted_targets[targets == -1] = 2  # X â†’ 2\n",
        "    converted_targets[targets == 0] = 0   # blank â†’ 0\n",
        "    converted_targets[targets == 1] = 1   # O â†’ 1\n",
        "    \n",
        "    # 9ê°œ ì…€ ê°ê°ì— ëŒ€í•œ ì†ì‹¤ ê³„ì‚°\n",
        "    total_loss = 0\n",
        "    for i in range(9):\n",
        "        cell_pred = predictions[:, i, :]       # ië²ˆì§¸ ì…€ ì˜ˆì¸¡ê°’\n",
        "        cell_target = converted_targets[:, i]  # ië²ˆì§¸ ì…€ ì‹¤ì œê°’\n",
        "        total_loss += loss_fn(cell_pred, cell_target)\n",
        "    \n",
        "    # í‰ê·  ì†ì‹¤ ë°˜í™˜\n",
        "    return total_loss / 9\n",
        "\n",
        "# ìµœì í™” í•¨ìˆ˜ ì„¤ì • - Adam\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugSpuxB27va_"
      },
      "source": [
        "#### 03. í•™ìŠµ ë° í‰ê°€: ì‹œê°í™” í•¨ìˆ˜, ì •í™•ë„ ì¸¡ì •, confusion matrix ë“±\n",
        "- í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì •í™•ë„ ì¸¡ì •\n",
        "- ì˜ˆì¸¡ì´ ì˜ ë˜ëŠ”ì§€ ì‹œê°í™”í•˜ì—¬ ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10]\n",
            "Train - Loss: 1.0883, Accuracy: 0.00%\n",
            "Valid - Loss: 1.0947, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [2/10]\n",
            "Train - Loss: 1.0491, Accuracy: 0.32%\n",
            "Valid - Loss: 1.1553, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [3/10]\n",
            "Train - Loss: 0.9636, Accuracy: 0.95%\n",
            "Valid - Loss: 1.5179, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [4/10]\n",
            "Train - Loss: 0.8803, Accuracy: 2.21%\n",
            "Valid - Loss: 1.9323, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [5/10]\n",
            "Train - Loss: 0.8020, Accuracy: 2.21%\n",
            "Valid - Loss: 1.7228, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [6/10]\n",
            "Train - Loss: 0.6962, Accuracy: 7.26%\n",
            "Valid - Loss: 1.1918, Accuracy: 7.46%\n",
            "--------------------------------------------------\n",
            "Epoch [7/10]\n",
            "Train - Loss: 0.6182, Accuracy: 11.36%\n",
            "Valid - Loss: 0.9554, Accuracy: 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch [8/10]\n",
            "Train - Loss: 0.5706, Accuracy: 12.30%\n",
            "Valid - Loss: 0.6089, Accuracy: 13.43%\n",
            "--------------------------------------------------\n",
            "Epoch [9/10]\n",
            "Train - Loss: 0.5227, Accuracy: 17.67%\n",
            "Valid - Loss: 0.5894, Accuracy: 14.93%\n",
            "--------------------------------------------------\n",
            "Epoch [10/10]\n",
            "Train - Loss: 0.4824, Accuracy: 23.97%\n",
            "Valid - Loss: 0.6120, Accuracy: 11.94%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        # í›ˆë ¨ ë‹¨ê³„\n",
        "        train_loss, train_acc = evaluate(model, train_loader, criterion, optimizer, is_training=True)\n",
        "        \n",
        "        # ê²€ì¦ ë‹¨ê³„\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, is_training=False)\n",
        "        \n",
        "        # í˜„ì¬ epoch ê²°ê³¼ ì¶œë ¥\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2%}\")\n",
        "        print(f\"Valid - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2%}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, criterion, optimizer=None, is_training=True):\n",
        "    model.train() if is_training else model.eval()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.set_grad_enabled(is_training):\n",
        "        for images, targets in data_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            # í•™ìŠµ ëª¨ë“œì¼ ê²½ìš° ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
        "            if is_training and optimizer:\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "            # ìˆœì „íŒŒ\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # í•™ìŠµ ëª¨ë“œì¼ ê²½ìš° ì—­ì „íŒŒ ë° ìµœì í™”\n",
        "            if is_training and optimizer:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # ì˜ˆì¸¡ ë° ì •í™•ë„ ê³„ì‚°\n",
        "            pred_classes = torch.argmax(outputs, dim=2)\n",
        "            converted_targets = targets.clone()\n",
        "            converted_targets[targets == -1] = 2  # Xë¥¼ 2ë¡œ ë³€í™˜\n",
        "            \n",
        "            correct += (pred_classes == converted_targets).all(dim=1).sum().item()\n",
        "            total += targets.size(0)\n",
        "    \n",
        "    return running_loss / len(data_loader), correct / total\n",
        "\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
        "num_epochs = 10\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader, \n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=ttt_loss_function,\n",
        "    num_epochs=num_epochs\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
