{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ë”¥ëŸ¬ë‹ ì‹¤ìŠµ ê³¼ì œ 3ì£¼ì°¨ - CNNì„ í™œìš©í•œ ì´ë¯¸ì§€ ë°ì´í„° ì¦ê°• ì‹¤í—˜\n",
        "\n",
        "ë‹¤ìŒ  ë„¤ ê°€ì§€ í™œë™ì„ í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "01. **ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”©**: \"ê¸°ì¡´\" vs \"ê¸°ì¡´ + ì¦ê°•\"\n",
        "02. **ëª¨ë¸ ì„¤ê³„**: CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ ì¬ì‚¬ìš© ë° ìˆ˜ì •\n",
        "03. **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜**: ê° ì¹¸ì— ëŒ€í•´ 3-í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "04. **ì¦ê°• ì ìš© ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ**"
      ],
      "metadata": {
        "id": "_vECysj7UNoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TTTDataset.zipì„ ë¶ˆëŸ¬ì™€ ë¬¸ì œì—ì„œ ìš”í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "\n",
        "ğŸ’¡ **ë°ì´í„° êµ¬ì¡°**  \n",
        "- **`image_black`** : ì´ë¯¸ì§€ ë°ì´í„°  \n",
        "- **`labels`** : íƒ€ê²Ÿ ë°ì´í„°  "
      ],
      "metadata": {
        "id": "s3_fdD7Y2iY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emp3WjnhSKg-",
        "outputId": "7fbaaceb-dabf-44ae-ac83-4a1fbdf84cfc"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import json\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "plOA2vEEKeEe"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BiN_OntoSVmU"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00. í´ë˜ìŠ¤\n",
        "ì •ì˜í•œ í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "MCpJfLtnagvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TTTDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, transform=None):\n",
        "        \"\"\"\n",
        "        í‹±íƒí†  ë°ì´í„°ì…‹ì„ PyTorch Dataset í˜•íƒœë¡œ ë³€í™˜.\n",
        "        :param image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "        :param label_paths: ë ˆì´ë¸” JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "        :param transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.transform = transform\n",
        "        self.data = self._load_data()\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\" ì´ë¯¸ì§€ & ë ˆì´ë¸” ë¡œë“œ \"\"\"\n",
        "        data = []\n",
        "        for img_path, lbl_path in zip(self.image_paths, self.label_paths):\n",
        "            # ì´ë¯¸ì§€ë¥¼ í‘ë°±(Grayscale)ë¡œ ë³€í™˜\n",
        "            image = Image.open(img_path).convert(\"L\")  # \"RGB\" ëŒ€ì‹  \"L\" ì‚¬ìš©\n",
        "\n",
        "            # JSON ë ˆì´ë¸” ë¡œë“œ\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                labels = json.load(f)\n",
        "\n",
        "            # ë ˆì´ë¸”ì„ ìˆ«ìë¡œ ë³€í™˜ (O=1, X=-1, blank=0)\n",
        "            label_tensor = torch.tensor(\n",
        "                [1 if v == \"O\" else -1 if v == \"X\" else 0 for v in labels.values()],\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "            data.append((image, label_tensor))\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜ \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" ë°ì´í„°ì…‹ì—ì„œ idx ë²ˆì§¸ ìƒ˜í”Œ(ì´ë¯¸ì§€ & ë ˆì´ë¸”)ì„ ê°€ì ¸ì˜¤ëŠ” ì—­í•  \"\"\"\n",
        "        image, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "Ymwd8gfCYHiq"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì••ì¶• íŒŒì¼ í’€ê¸°\n",
        "zip_path = \"/content/drive/MyDrive/TTTDataset.zip\"\n",
        "extract_path = \"/content/TTTDataset\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    !unzip -q \"{zip_path}\" -d \"/content/\"\n",
        "    print(\" ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\" ì´ë¯¸ ì••ì¶•ì´ í’€ë ¤ ìˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IW-GSVASQO6",
        "outputId": "1c81f939-bbdc-4c91-9296-36a7b4deff6c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì´ë¯¸ ì••ì¶•ì´ í’€ë ¤ ìˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”©: \"ê¸°ì¡´\" vs \"ê¸°ì¡´ + ì¦ê°•\"\n",
        "\n",
        "ğŸ’¡ ì‹¤í—˜ì„ ìœ„í•œ **ë‘ ê°œì˜ DataLoader**ë¥¼ êµ¬ì„±í•˜ì„¸ìš” (1ì£¼ì°¨ ê³¼ì œ ì°¸ê³ )\n",
        "- ê¸°ì¡´ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ” DataLoader\n",
        "- ê¸°ì¡´ ë°ì´í„° + ì‹¤ì‹œê°„ transform ì¦ê°•ì„ ì ìš©í•œ DataLoader\n"
      ],
      "metadata": {
        "id": "seQgBNd9aZNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "WGUrhGc2DqJC"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¯¸ì§€ ë° ë ˆì´ë¸” ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
        "image_dir = \"/content/TTTDataset/image_black\"\n",
        "label_dir = \"/content/TTTDataset/labels\"\n",
        "\n",
        "# ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ìë™ ìˆ˜ì§‘ (í™•ì¥ìê°€ .jpg ë˜ëŠ” .JPGì¸ ì  í™•ì¸)\n",
        "image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")) +\n",
        "                     glob.glob(os.path.join(image_dir, \"*.JPG\")))\n",
        "\n",
        "label_paths = sorted(glob.glob(os.path.join(label_dir, \"*.json\")))"
      ],
      "metadata": {
        "id": "Ax4a1k3uH2TF"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê°ì ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤í–‰!"
      ],
      "metadata": {
        "id": "5iKVVvS1UiEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ë³¸ transform\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(), # í”½ì…€ê°’ì„ í…ì„œë¡œ ë³€í™˜ (0~1ë¡œ ìë™ ìŠ¤ì¼€ì¼ë§ë¨)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# ì¦ê°• transform\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomRotation(15), # ì£¼ì–´ì§„ ê°ë„ì— ëŒ€í•´ ëœë¤í•˜ê²Œ íšŒì „\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3), # ì»¬ëŸ¬ ê´€ë ¨ ì†ì„± ë³€ê²½\n",
        "    transforms.GaussianBlur(3), # ê° í”½ì…€ ì£¼ë³€ì˜ ê°’ì„ í‰ê·  ë‚´ì–´ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬ / ë…¸ì´ì¦ˆ ì œê±°\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "IRSolv5kUKLd"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê°ì Dataset ë° Dataloader ì •ì˜!"
      ],
      "metadata": {
        "id": "0ZCBpZZiX2xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ë³¸ Dataset ìƒì„±\n",
        "base_dataset = TTTDataset(image_paths, label_paths, transform=base_transform)\n",
        "\n",
        "# ë¶„í•  ë¹„ìœ¨ ì„¤ì • (ì˜ˆ: 70% train, 15% val, 15% test)\n",
        "total_size = len(base_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# ê¸°ë³¸ Dataset ë¶„í• \n",
        "train_base_dataset, val_base_dataset, test_base_dataset = random_split(base_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# ê¸°ë³¸ DataLoader ìƒì„±\n",
        "train_loader = DataLoader(train_base_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_base_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_base_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "5L8C_4kLaI19"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¡ **ì¦ê°•**ì€ \"ì‹¤ì‹œê°„ ë³€í˜•\"ì„ ëœ»í•œë‹¤.\n",
        "- ë™ì¼í•œ ì´ë¯¸ì§€ ê²½ë¡œ/ë ˆì´ë¸” ê²½ë¡œë¥¼ ì°¸ì¡°í•˜ê¸° ë•Œë¬¸ì— ê°œìˆ˜ê°€ ì™„ì „íˆ ë™ì¼í•˜ë‹¤! ë”°ë¼ì„œ **ë°ì´í„° ê°œìˆ˜**ê°€ ëŠ˜ì–´ë‚˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ê°™ì€ ê°œìˆ˜ì˜ ë°ì´í„°ë¥¼ ë§¤ë²ˆ **ë‹¤ë¥´ê²Œ ë³€í˜•í•´ì„œ í•™ìŠµì— ì‚¬ìš©**í•œë‹¤ëŠ” ì ~\n",
        "-  **`Subset(aug_dataset, train_base_dataset.indices)`**ì„ ì“°ëŠ” ì´ìœ ëŠ” **ê°™ì€ ë°ì´í„° ë¶„í•  ê¸°ì¤€ì„ ìœ ì§€í•˜ë©´ì„œ transformë§Œ ë‹¤ë¥´ê²Œ í•˜ê¸° ìœ„í•¨**ì´ë‹¤!\n",
        "- aug_datasetë„ **`random_split`**ìœ¼ë¡œ ë‚˜ëˆ ë²„ë¦¬ë©´ **ê¸°ì¡´ í•™ìŠµìš© ë°ì´í„°ì™€ ì „í˜€ ë‹¤ë¥¸ ë°ì´í„°ë¡œ í•™ìŠµ**í•˜ê²Œ ë¼ì„œ **transformì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤!**"
      ],
      "metadata": {
        "id": "pKlbOdT2gUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¦ê°• Dataset ìƒì„±\n",
        "aug_dataset = TTTDataset(image_paths, label_paths, transform=aug_transform)\n",
        "\n",
        "# ì¦ê°• Dataset ë¶„í•  (ì´ë•Œ random_splitì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë™ì¼í•œ ë¶„í•  ì¸ë±ìŠ¤ë¥¼ í™œìš©í•œë‹¤.)\n",
        "train_aug_dataset = torch.utils.data.Subset(aug_dataset, train_base_dataset.indices)\n",
        "\n",
        "# ì¦ê°• DataLoader ìƒì„± (ì´ë•Œ val_loader ì™€ test_loaderëŠ” ê¸°ë³¸ Datasetì˜ ê²ƒì„ ì°¨ìš©í•˜ê¸° ë•Œë¬¸ì— ë³„ë„ë¡œ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.)\n",
        "train_aug_loader = DataLoader(train_aug_dataset, batch_size=32, shuffle=True)\n",
        "'''\n",
        "val_loader = DataLoader(val_base_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_base_dataset, batch_size=32, shuffle=True)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n7PyLLwSc_K_",
        "outputId": "aca9577d-dfe0-49c7-df8f-bc634f4d45fa"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nval_loader = DataLoader(val_base_dataset, batch_size=32, shuffle=True)\\ntest_loader = DataLoader(test_base_dataset, batch_size=32, shuffle=True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02. ëª¨ë¸ ì„¤ê³„: CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ ì¬ì‚¬ìš© ë° ìˆ˜ì •\n",
        "- 1ì±„ë„ ì´ë¯¸ì§€ ì…ë ¥, 9 Ã— 3-í´ë˜ìŠ¤ ì¶œë ¥ êµ¬ì¡° ìœ ì§€ (2ì£¼ì°¨ ê³¼ì œ ì°¸ê³ )\n",
        "- Dropout/Hidden Layer ë“± ìˆ˜ì • ê°€ëŠ¥\n"
      ],
      "metadata": {
        "id": "DYVGt8R27tXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TicTacToeCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TicTacToeCNN, self).__init__()\n",
        "        # ì²«ë²ˆì§¸ ì¸µ: 1ì±„ë„ -> 32ì±„ë„, BatchNorm ì¶”ê°€\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # (B, 32, 64, 64)\n",
        "        )\n",
        "\n",
        "        # ë‘ë²ˆì§¸ ì¸µ: 32ì±„ë„ -> 64ì±„ë„, BatchNorm ì¶”ê°€\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # (B, 64, 32, 32)\n",
        "        )\n",
        "\n",
        "        # ì„¸ë²ˆì§¸ ì¸µ: 64ì±„ë„ -> 128ì±„ë„, BatchNorm ì¶”ê°€\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))           # (B, 128, 1, 1)\n",
        "        )\n",
        "\n",
        "        # Fully-connected Layer: í™•ì¥ëœ êµ¬ì¡°ë¡œ ìˆ˜ì •\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),                       # Dropout ë¹„ìœ¨ ì•½ê°„ ì¦ê°€\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 9 * 3)                   # ìµœì¢… ì¶œë ¥: 9ì¹¸ x 3 í´ë˜ìŠ¤\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)              # (B, 32, 64, 64)\n",
        "        x = self.layer2(x)              # (B, 64, 32, 32)\n",
        "        x = self.layer3(x)              # (B, 128, 1, 1)\n",
        "        x = x.view(x.size(0), -1)       # (B, 128)\n",
        "        x = self.fc_layer(x)            # (B, 27)\n",
        "        return x.view(-1, 9, 3)         # (B, 9, 3)"
      ],
      "metadata": {
        "id": "yHA859eaDosf"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜: ê° ì¹¸ì— ëŒ€í•´ 3-í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì´ë¯€ë¡œ CrossEntropyLoss ì‚¬ìš© ê°€ëŠ¥\n",
        "- 9ê°œ ì¹¸ì„ ê°ê° ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ êµ¬ì„±\n",
        "- Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© ê¶Œì¥"
      ],
      "metadata": {
        "id": "ugSpuxB27va_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ëª¨ë¸ ê°ì²´ ì„ ì–¸\n",
        "model = TicTacToeCNN().to(device)\n",
        "\n",
        "# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "st1wMoa8Dnwt"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04. ì¦ê°• ì ìš© ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ\n",
        "- ë‘ ê°œì˜ DataLoaderë¡œ ë‘ ë²ˆ ì‹¤í—˜\n",
        "- ê°™ì€ ëª¨ë¸ êµ¬ì¡°ë¡œ í•™ìŠµ â†’ ì„±ëŠ¥ ì°¨ì´ ë¹„êµ (ì •í™•ë„/í•™ìŠµ ì•ˆì •ì„±/ê³¼ì í•© ì—¬ë¶€ ë“±)"
      ],
      "metadata": {
        "id": "cF58k_5VDhv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "def preprocess_batch(images, labels):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # CrossEntropyëŠ” ì •ìˆ˜ í´ë˜ìŠ¤ (0, 1, 2)ë¥¼ ê¸°ëŒ€í•¨\n",
        "    target = (labels + 1).to(torch.int64)   # -1 â†’ 0, 0 â†’ 1, 1 â†’ 2ë¡œ ì •ìˆ˜í˜• ë³€í™˜\n",
        "    return images, target"
      ],
      "metadata": {
        "id": "T33wjZwADl7h"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
        "def train_model(model, loader, optimizer, criterion, val_loader, epochs=30):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "      # ëˆ„ì  ì†ì‹¤ ì´ˆê¸°í™”\n",
        "      total_loss = 0\n",
        "\n",
        "      for images, labels in loader:\n",
        "          images, target = preprocess_batch(images, labels)\n",
        "\n",
        "          optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
        "          outputs = model(images)  # (B, 9, 3)\n",
        "          loss = criterion(outputs.view(-1, 3), target.view(-1)) # ì†ì‹¤ ê³„ì‚°\n",
        "          loss.backward() # ì—­ì „íŒŒ\n",
        "          optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
        "          total_loss += loss.item() # ì†ì‹¤ ëˆ„ì \n",
        "      val_acc = evaluate(model, val_loader)\n",
        "      print(f\"[Epoch {epoch+1:02d}] Loss: {total_loss:.4f} | Val Accuracy: {val_acc:.2%}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "QVaUCQPaoMJG"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "def evaluate(model, loader):  # train_loader, val_loader, test_loader ì¤‘ í•˜ë‚˜ë¥¼ ë„˜ê¸¸ ìˆ˜ ìˆìŒ\n",
        "    model.eval()\n",
        "    # ì˜ˆì¸¡ì´ ë§ì€ ì¹¸ ìˆ˜\n",
        "    correct = 0\n",
        "    # ì „ì²´ ì¹¸ ìˆ˜\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, target = preprocess_batch(images, labels)\n",
        "            # ëª¨ë¸ì— ì´ë¯¸ì§€ ì‚½ì…í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "            outputs = model(images)  # (B, 9, 3)\n",
        "            # ê° ì¹¸ë§ˆë‹¤ 3-í´ë˜ìŠ¤ ì¤‘ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì„ íƒ\n",
        "            preds = outputs.argmax(dim=2)  # (B, 9)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.numel()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "v4u0tgErn96P"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`get_new_model()`**ì€ ë‘ ì‹¤í—˜ì˜ ì¡°ê±´ì„ ê³µì •í•˜ê²Œ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ ê¼­ í•„ìš”í•œ í•¨ìˆ˜ì´ë‹¤.\n",
        "- ìš°ë¦¬ê°€ ë¹„êµí•˜ë ¤ëŠ” ê±´ **ê¸°ì¡´ ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸ vs ì¦ê°• ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸** ì´ê¸° ë•Œë¬¸ì—, ë§¤ ì‹¤í—˜ë§ˆë‹¤ **\"ìƒˆ ëª¨ë¸\"**ë¡œ ì‹œì‘í•´ì•¼ í•œë‹¤!\n",
        "- ëª¨ë¸ì„ ìƒˆë¡œ ë§Œë“¤ì§€ ì•ŠëŠ”ë‹¤ë©´ ë‘ ë²ˆì§¸ í•™ìŠµì´ ì²« ë²ˆì§¸ ê²°ê³¼ ìœ„ì— ë®ì–´ì”Œì›Œì ¸ì„œ ì‹¤í—˜ ê²°ê³¼ê°€ ì™œê³¡ëœë‹¤."
      ],
      "metadata": {
        "id": "I573LDL5_8qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ì •ì˜\n",
        "def get_new_model():\n",
        "    model = TicTacToeCNN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return model, optimizer, criterion"
      ],
      "metadata": {
        "id": "gt3ysd31Atoj"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ê¸°ì¡´ ë°ì´í„° vs ì¦ê°• ë°ì´í„°**ì— ëŒ€í•´ ê°ê° í•™ìŠµí•˜ê³ ,\n",
        "- **ë™ì¼í•œ ê²€ì¦ì…‹**ì—ì„œ ìµœì¢… ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì½”ë“œ"
      ],
      "metadata": {
        "id": "TP-82KVlBK06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‹¤í—˜ 1: ì¦ê°• ì—†ì´ ê¸°ë³¸ ì´ë¯¸ì§€ í•™ìŠµ\n",
        "print(\"ì‹¤í—˜ 1: ì¦ê°• ì—†ì´ ê¸°ë³¸ ì´ë¯¸ì§€ í•™ìŠµ\")\n",
        "model1, optimizer1, criterion1 = get_new_model() # ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™”\n",
        "model1 = train_model(model1, train_loader, optimizer1, criterion1, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OQKcy1WqQNZ",
        "outputId": "83c97708-1af0-4f16-ebc9-719a883a50c1"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‹¤í—˜ 1: ì¦ê°• ì—†ì´ ê¸°ë³¸ ì´ë¯¸ì§€ í•™ìŠµ\n",
            "[Epoch 01] Loss: 10.8884 | Val Accuracy: 35.99%\n",
            "[Epoch 02] Loss: 10.7427 | Val Accuracy: 40.30%\n",
            "[Epoch 03] Loss: 10.5583 | Val Accuracy: 41.29%\n",
            "[Epoch 04] Loss: 10.2593 | Val Accuracy: 46.43%\n",
            "[Epoch 05] Loss: 9.8438 | Val Accuracy: 49.92%\n",
            "[Epoch 06] Loss: 9.4804 | Val Accuracy: 49.09%\n",
            "[Epoch 07] Loss: 9.2561 | Val Accuracy: 52.57%\n",
            "[Epoch 08] Loss: 9.1575 | Val Accuracy: 48.26%\n",
            "[Epoch 09] Loss: 9.0450 | Val Accuracy: 50.75%\n",
            "[Epoch 10] Loss: 8.9832 | Val Accuracy: 50.08%\n",
            "[Epoch 11] Loss: 8.9060 | Val Accuracy: 49.75%\n",
            "[Epoch 12] Loss: 8.9235 | Val Accuracy: 51.91%\n",
            "[Epoch 13] Loss: 8.8746 | Val Accuracy: 49.59%\n",
            "[Epoch 14] Loss: 8.7892 | Val Accuracy: 51.74%\n",
            "[Epoch 15] Loss: 8.7273 | Val Accuracy: 51.91%\n",
            "[Epoch 16] Loss: 8.6471 | Val Accuracy: 50.91%\n",
            "[Epoch 17] Loss: 8.5947 | Val Accuracy: 50.41%\n",
            "[Epoch 18] Loss: 8.6138 | Val Accuracy: 51.58%\n",
            "[Epoch 19] Loss: 8.6093 | Val Accuracy: 51.08%\n",
            "[Epoch 20] Loss: 8.6916 | Val Accuracy: 53.57%\n",
            "[Epoch 21] Loss: 8.5716 | Val Accuracy: 51.74%\n",
            "[Epoch 22] Loss: 8.5382 | Val Accuracy: 52.90%\n",
            "[Epoch 23] Loss: 8.4762 | Val Accuracy: 52.74%\n",
            "[Epoch 24] Loss: 8.4323 | Val Accuracy: 52.07%\n",
            "[Epoch 25] Loss: 8.3992 | Val Accuracy: 53.07%\n",
            "[Epoch 26] Loss: 8.4339 | Val Accuracy: 49.92%\n",
            "[Epoch 27] Loss: 8.5185 | Val Accuracy: 51.58%\n",
            "[Epoch 28] Loss: 8.4948 | Val Accuracy: 53.57%\n",
            "[Epoch 29] Loss: 8.4031 | Val Accuracy: 52.40%\n",
            "[Epoch 30] Loss: 8.4038 | Val Accuracy: 52.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‹¤í—˜ 2: ì¦ê°• ì´ë¯¸ì§€ í•™ìŠµ\n",
        "print(\"ì‹¤í—˜ 2: ì¦ê°• ì´ë¯¸ì§€ í•™ìŠµ\")\n",
        "model2, optimizer2, criterion2 = get_new_model()\n",
        "model2 = train_model(model2, train_aug_loader, optimizer2, criterion2, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpWkK45BqS4E",
        "outputId": "d9808f4e-2d12-4b0e-a1b6-39ce6c153527"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‹¤í—˜ 2: ì¦ê°• ì´ë¯¸ì§€ í•™ìŠµ\n",
            "[Epoch 01] Loss: 10.8779 | Val Accuracy: 37.15%\n",
            "[Epoch 02] Loss: 10.7709 | Val Accuracy: 40.96%\n",
            "[Epoch 03] Loss: 10.4949 | Val Accuracy: 43.12%\n",
            "[Epoch 04] Loss: 10.1200 | Val Accuracy: 47.26%\n",
            "[Epoch 05] Loss: 9.6990 | Val Accuracy: 52.24%\n",
            "[Epoch 06] Loss: 9.5037 | Val Accuracy: 51.74%\n",
            "[Epoch 07] Loss: 9.2761 | Val Accuracy: 52.07%\n",
            "[Epoch 08] Loss: 9.2268 | Val Accuracy: 51.91%\n",
            "[Epoch 09] Loss: 9.1988 | Val Accuracy: 51.24%\n",
            "[Epoch 10] Loss: 9.1362 | Val Accuracy: 52.57%\n",
            "[Epoch 11] Loss: 9.0205 | Val Accuracy: 51.24%\n",
            "[Epoch 12] Loss: 9.0104 | Val Accuracy: 52.90%\n",
            "[Epoch 13] Loss: 8.9514 | Val Accuracy: 51.24%\n",
            "[Epoch 14] Loss: 8.9055 | Val Accuracy: 52.74%\n",
            "[Epoch 15] Loss: 8.8935 | Val Accuracy: 52.07%\n",
            "[Epoch 16] Loss: 8.8462 | Val Accuracy: 50.75%\n",
            "[Epoch 17] Loss: 8.8554 | Val Accuracy: 52.07%\n",
            "[Epoch 18] Loss: 8.8442 | Val Accuracy: 49.42%\n",
            "[Epoch 19] Loss: 8.8196 | Val Accuracy: 51.41%\n",
            "[Epoch 20] Loss: 8.8124 | Val Accuracy: 53.90%\n",
            "[Epoch 21] Loss: 8.7590 | Val Accuracy: 52.24%\n",
            "[Epoch 22] Loss: 8.6796 | Val Accuracy: 53.57%\n",
            "[Epoch 23] Loss: 8.7446 | Val Accuracy: 52.07%\n",
            "[Epoch 24] Loss: 8.6483 | Val Accuracy: 50.08%\n",
            "[Epoch 25] Loss: 8.6625 | Val Accuracy: 54.06%\n",
            "[Epoch 26] Loss: 8.6376 | Val Accuracy: 53.23%\n",
            "[Epoch 27] Loss: 8.6041 | Val Accuracy: 51.91%\n",
            "[Epoch 28] Loss: 8.6340 | Val Accuracy: 55.39%\n",
            "[Epoch 29] Loss: 8.5811 | Val Accuracy: 53.90%\n",
            "[Epoch 30] Loss: 8.6028 | Val Accuracy: 51.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ìµœì¢… ë¹„êµ: ë‘ ëª¨ë¸ì„ ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€\n",
        "final_acc1 = evaluate(model1, val_loader)\n",
        "final_acc2 = evaluate(model2, val_loader)\n",
        "print(f\"Final Validation Accuracy (Base): {final_acc1:.2%}\")\n",
        "print(f\"Final Validation Accuracy (Augmented): {final_acc2:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fipF9LiwBp22",
        "outputId": "15aaef7b-1fc1-458c-c4ea-6cbc5e460603"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy (Base): 52.24%\n",
            "Final Validation Accuracy (Augmented): 51.74%\n"
          ]
        }
      ]
    }
  ]
}