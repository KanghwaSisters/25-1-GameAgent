{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ë”¥ëŸ¬ë‹ ì‹¤ìŠµ ê³¼ì œ 2ì£¼ì°¨ - CNNì„ í™œìš©í•˜ì—¬ ë³´ë“œ ì „ì²´ë¥¼ ì…ë ¥ë°›ì•„ 9ì¹¸ ìƒíƒœ ì˜ˆì¸¡\n",
        "\n",
        "ë‹¤ìŒ  ì„¸ ê°€ì§€ í™œë™ì„ í•´ë´…ì‹œë‹¤.\n",
        "\n",
        "01. **ëª¨ë¸ ì„¤ê³„**: CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ í™œìš©\n",
        "02. **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜**: ê° ì¹¸ì— ëŒ€í•´ 3-í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "03. **í•™ìŠµ ë° í‰ê°€**: ì‹œê°í™” í•¨ìˆ˜, ì •í™•ë„ ì¸¡ì •, confusion matrix ë“±"
      ],
      "metadata": {
        "id": "_vECysj7UNoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TTTDataset.zipì„ ë¶ˆëŸ¬ì™€ ë¬¸ì œì—ì„œ ìš”í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "\n",
        "ğŸ’¡ **ë°ì´í„° êµ¬ì¡°**  \n",
        "- **`image_black`** : ì´ë¯¸ì§€ ë°ì´í„°  \n",
        "- **`labels`** : íƒ€ê²Ÿ ë°ì´í„°  "
      ],
      "metadata": {
        "id": "s3_fdD7Y2iY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BJKliYMHk2q",
        "outputId": "d852e7e6-870c-45bc-8a28-043d65457c79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "BhRH2q-fV8lc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00. í´ë˜ìŠ¤\n",
        "ì •ì˜í•œ í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
      ],
      "metadata": {
        "id": "MCpJfLtnagvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TTTDataset(Dataset):\n",
        "    def __init__(self, image_paths, label_paths, transform=None):\n",
        "        \"\"\"\n",
        "        í‹±íƒí†  ë°ì´í„°ì…‹ì„ PyTorch Dataset í˜•íƒœë¡œ ë³€í™˜.\n",
        "        :param image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "        :param label_paths: ë ˆì´ë¸” JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "        :param transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.transform = transform\n",
        "        self.data = self._load_data()\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\" ì´ë¯¸ì§€ & ë ˆì´ë¸” ë¡œë“œ \"\"\"\n",
        "        data = []\n",
        "        for img_path, lbl_path in zip(self.image_paths, self.label_paths):\n",
        "            # ì´ë¯¸ì§€ë¥¼ í‘ë°±(Grayscale)ë¡œ ë³€í™˜\n",
        "            image = Image.open(img_path).convert(\"L\")  # \"RGB\" ëŒ€ì‹  \"L\" ì‚¬ìš©\n",
        "\n",
        "            # JSON ë ˆì´ë¸” ë¡œë“œ\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                labels = json.load(f)\n",
        "\n",
        "            # ë ˆì´ë¸”ì„ ìˆ«ìë¡œ ë³€í™˜ (O=1, X=-1, blank=0)\n",
        "            label_tensor = torch.tensor(\n",
        "                [1 if v == \"O\" else -1 if v == \"X\" else 0 for v in labels.values()],\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "            data.append((image, label_tensor))\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜ \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" ë°ì´í„°ì…‹ì—ì„œ idx ë²ˆì§¸ ìƒ˜í”Œ(ì´ë¯¸ì§€ & ë ˆì´ë¸”)ì„ ê°€ì ¸ì˜¤ëŠ” ì—­í•  \"\"\"\n",
        "        image, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "Ymwd8gfCYHiq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì••ì¶• íŒŒì¼ í’€ê¸°\n",
        "zip_path = \"/content/drive/MyDrive/TTTDataset.zip\"\n",
        "extract_path = \"/content/TTTDataset\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    !unzip -q \"{zip_path}\" -d \"/content/\"\n",
        "    print(\" ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\" ì´ë¯¸ ì••ì¶•ì´ í’€ë ¤ ìˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "kYqOL0NrIfEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743d4ead-b432-4875-9a7d-e9a2a3a29d65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì••ì¶• í•´ì œ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¯¸ì§€ ë° ë ˆì´ë¸” ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
        "image_dir = \"/content/TTTDataset/image_black\"\n",
        "label_dir = \"/content/TTTDataset/labels\"\n",
        "\n",
        "# ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ìë™ ìˆ˜ì§‘ (í™•ì¥ìê°€ .jpg ë˜ëŠ” .JPGì¸ ì  í™•ì¸)\n",
        "image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")) +\n",
        "                     glob.glob(os.path.join(image_dir, \"*.JPG\")))\n",
        "\n",
        "label_paths = sorted(glob.glob(os.path.join(label_dir, \"*.json\")))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(), # í”½ì…€ê°’ì„ í…ì„œë¡œ ë³€í™˜ (0~1ë¡œ ìë™ ìŠ¤ì¼€ì¼ë§ë¨)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Dataset ìƒì„±\n",
        "dataset = TTTDataset(image_paths, label_paths, transform=transform)\n",
        "\n",
        "# ì „ì²´ ë°ì´í„° í¬ê¸° ê¸°ì¤€ ë¶„í•  ë¹„ìœ¨ ì„¤ì • (ì˜ˆ: 70% train, 15% val, 15% test)\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# ë°ì´í„° ë¶„í• \n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# ê°ê°ì— ëŒ€í•œ ë°ì´í„°ë¡œë” ìƒì„±\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "Ue-8ZaunXd1q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01. ëª¨ë¸ ì„¤ê³„: CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ í™œìš©\n",
        "- ì…ë ¥: í‹±íƒí†  ë³´ë“œ ì „ì²´ ì´ë¯¸ì§€ (ì˜ˆ: 128x128)\n",
        "- ì¶œë ¥: 9ê°œì˜ ìƒíƒœ(ê° ì¹¸ë§ˆë‹¤ O, X, blank ì¤‘ í•˜ë‚˜)"
      ],
      "metadata": {
        "id": "seQgBNd9aZNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8KvefRQ4XhtC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        # super(): CNN classì˜ ë¶€ëª¨ classì¸ nn.Moduleì„ ì´ˆê¸°í™”\n",
        "        super(TicTacToeCNN, self).__init__()\n",
        "\n",
        "        # ì²«ë²ˆì§¸ ì¸µ\n",
        "        self.layer1 = nn.Sequential(\n",
        "            # Convolution Layer: 1ì±„ë„ ì…ë ¥ì„ 32ì±„ë„ íŠ¹ì§•ë§µ(feature map)ìœ¼ë¡œ ë³€í™˜\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),  # (B, 1, 128, 128) â†’ (B, 16, 128, 128)\n",
        "            # ë¹„ì„ í˜• í™œì„±í•¨ìˆ˜\n",
        "            nn.ReLU(),\n",
        "            # Pooling Layer: í¬ê¸° ì ˆë°˜ìœ¼ë¡œ ì¶•ì†Œ\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))            # (B, 32, 64, 64)\n",
        "\n",
        "        # ë‘ë²ˆì§¸ ì¸µ\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))              # (B, 64, 32, 32)\n",
        "\n",
        "        # ì„¸ë²ˆì§¸ ì¸µ\n",
        "        self.layer3= nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))    # (B, 128, 1, 1)\n",
        "        )\n",
        "\n",
        "        # Fully-connected Layer: ì¹¸ ë³„ë¡œ 3-í´ë˜ìŠ¤ ì ìˆ˜ ì¶œë ¥\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(128, 256),   # ì¤‘ê°„ hidden layer ì¶”ê°€\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9 * 3))   # 128ì°¨ì› ë°±í„° -> 27ê°œ ì¶œë ¥ (9ì¹¸ Ã— ê° ì¹¸ ë‹¹ 3-í´ë˜ìŠ¤)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)              # (B, 16, 64, 64)\n",
        "        x = self.layer2(x)              # (B, 32, 32, 32)\n",
        "        x = self.layer3(x)              # (B, 64, 1, 1)\n",
        "        x = x.view(x.size(0), -1)       # (B, 64)\n",
        "        x = self.fc_layer(x)            # (B, 27)\n",
        "        return x.view(-1, 9, 3)         # (B, 9, 3)"
      ],
      "metadata": {
        "id": "misG2sDL1pj1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜: ê° ì¹¸ì— ëŒ€í•´ 3-í´ë˜ìŠ¤ ë¶„ë¥˜\n",
        "- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì´ë¯€ë¡œ CrossEntropyLoss ì‚¬ìš© ê°€ëŠ¥\n",
        "- 9ê°œ ì¹¸ì„ ê°ê° ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ êµ¬ì„±"
      ],
      "metadata": {
        "id": "DYVGt8R27tXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ëª¨ë¸ ê°ì²´ ì„ ì–¸\n",
        "model = TicTacToeCNN().to(device)\n",
        "\n",
        "# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "aHRXEG9NB4kX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03. í•™ìŠµ ë° í‰ê°€: ì‹œê°í™” í•¨ìˆ˜, ì •í™•ë„ ì¸¡ì •, confusion matrix ë“±\n",
        "- í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì •í™•ë„ ì¸¡ì •\n",
        "- ì˜ˆì¸¡ì´ ì˜ ë˜ëŠ”ì§€ ì‹œê°í™”í•˜ì—¬ ë¶„ì„"
      ],
      "metadata": {
        "id": "ugSpuxB27va_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "def preprocess_batch(images, labels):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # CrossEntropyëŠ” ì •ìˆ˜ í´ë˜ìŠ¤ (0, 1, 2)ë¥¼ ê¸°ëŒ€í•¨\n",
        "    target = (labels + 1).to(torch.int64)   # -1 â†’ 0, 0 â†’ 1, 1 â†’ 2ë¡œ ì •ìˆ˜í˜• ë³€í™˜\n",
        "    return images, target"
      ],
      "metadata": {
        "id": "MO0MYs9MwHNl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "def evaluate(loader):  # train_loader, val_loader, test_loader ì¤‘ í•˜ë‚˜ë¥¼ ë„˜ê¸¸ ìˆ˜ ìˆìŒ\n",
        "    model.eval()\n",
        "    # ì˜ˆì¸¡ì´ ë§ì€ ì¹¸ ìˆ˜\n",
        "    correct = 0\n",
        "    # ì „ì²´ ì¹¸ ìˆ˜\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, target = preprocess_batch(images, labels)\n",
        "            # ëª¨ë¸ì— ì´ë¯¸ì§€ ì‚½ì…í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "            outputs = model(images)  # (B, 9, 3)\n",
        "            # ê° ì¹¸ë§ˆë‹¤ 3-í´ë˜ìŠ¤ ì¤‘ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì„ íƒ\n",
        "            preds = outputs.argmax(dim=2)  # (B, 9)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.numel()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "Sq_Dv7rO16lx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    # ëˆ„ì  ì†ì‹¤ ì´ˆê¸°í™”\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, target = preprocess_batch(images, labels)\n",
        "\n",
        "        optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
        "        outputs = model(images)  # (B, 9, 3)\n",
        "        loss = criterion(outputs.view(-1, 3), target.view(-1)) # ì†ì‹¤ ê³„ì‚°\n",
        "        loss.backward() # ì—­ì „íŒŒ\n",
        "        optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
        "        total_loss += loss.item() # ì†ì‹¤ ëˆ„ì \n",
        "\n",
        "    # 1 epochë§ˆë‹¤ ê²€ì¦ ì •í™•ë„ í‰ê°€\n",
        "    val_acc = evaluate(val_loader)\n",
        "    print(f\"[Epoch {epoch+1:02d}] Loss: {total_loss:.4f} | Val Accuracy: {val_acc:.2%}\")"
      ],
      "metadata": {
        "id": "b2xM8ol5Xlj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dc776e-9ffa-4753-c7cc-48f4c2ec82a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] Loss: 10.9935 | Val Accuracy: 38.14%\n",
            "[Epoch 02] Loss: 10.8863 | Val Accuracy: 41.13%\n",
            "[Epoch 03] Loss: 10.8382 | Val Accuracy: 39.30%\n",
            "[Epoch 04] Loss: 10.7715 | Val Accuracy: 43.12%\n",
            "[Epoch 05] Loss: 10.6664 | Val Accuracy: 41.46%\n",
            "[Epoch 06] Loss: 10.6633 | Val Accuracy: 40.63%\n",
            "[Epoch 07] Loss: 10.5939 | Val Accuracy: 47.26%\n",
            "[Epoch 08] Loss: 10.4820 | Val Accuracy: 47.76%\n",
            "[Epoch 09] Loss: 10.4154 | Val Accuracy: 46.60%\n",
            "[Epoch 10] Loss: 10.2621 | Val Accuracy: 45.77%\n",
            "[Epoch 11] Loss: 10.1795 | Val Accuracy: 46.43%\n",
            "[Epoch 12] Loss: 10.1407 | Val Accuracy: 45.77%\n",
            "[Epoch 13] Loss: 10.2034 | Val Accuracy: 44.78%\n",
            "[Epoch 14] Loss: 10.0525 | Val Accuracy: 44.78%\n",
            "[Epoch 15] Loss: 9.9779 | Val Accuracy: 45.61%\n",
            "[Epoch 16] Loss: 9.8830 | Val Accuracy: 45.94%\n",
            "[Epoch 17] Loss: 9.8448 | Val Accuracy: 46.43%\n",
            "[Epoch 18] Loss: 9.7662 | Val Accuracy: 46.77%\n",
            "[Epoch 19] Loss: 9.6980 | Val Accuracy: 45.61%\n",
            "[Epoch 20] Loss: 9.6574 | Val Accuracy: 44.94%\n",
            "[Epoch 21] Loss: 9.5902 | Val Accuracy: 46.43%\n",
            "[Epoch 22] Loss: 9.5636 | Val Accuracy: 50.58%\n",
            "[Epoch 23] Loss: 9.4689 | Val Accuracy: 51.41%\n",
            "[Epoch 24] Loss: 9.3622 | Val Accuracy: 50.75%\n",
            "[Epoch 25] Loss: 9.1761 | Val Accuracy: 51.24%\n",
            "[Epoch 26] Loss: 9.0513 | Val Accuracy: 49.59%\n",
            "[Epoch 27] Loss: 9.0415 | Val Accuracy: 51.24%\n",
            "[Epoch 28] Loss: 8.9308 | Val Accuracy: 50.25%\n",
            "[Epoch 29] Loss: 8.8712 | Val Accuracy: 50.91%\n",
            "[Epoch 30] Loss: 8.7814 | Val Accuracy: 50.25%\n",
            "[Epoch 31] Loss: 8.7529 | Val Accuracy: 51.41%\n",
            "[Epoch 32] Loss: 8.7519 | Val Accuracy: 50.91%\n",
            "[Epoch 33] Loss: 8.7124 | Val Accuracy: 50.08%\n",
            "[Epoch 34] Loss: 8.6943 | Val Accuracy: 50.25%\n",
            "[Epoch 35] Loss: 8.6625 | Val Accuracy: 50.41%\n",
            "[Epoch 36] Loss: 8.6559 | Val Accuracy: 49.59%\n",
            "[Epoch 37] Loss: 8.6367 | Val Accuracy: 50.25%\n",
            "[Epoch 38] Loss: 8.6012 | Val Accuracy: 51.08%\n",
            "[Epoch 39] Loss: 8.5621 | Val Accuracy: 49.75%\n",
            "[Epoch 40] Loss: 8.5571 | Val Accuracy: 51.58%\n",
            "[Epoch 41] Loss: 8.5441 | Val Accuracy: 49.42%\n",
            "[Epoch 42] Loss: 8.5576 | Val Accuracy: 50.75%\n",
            "[Epoch 43] Loss: 8.6033 | Val Accuracy: 51.74%\n",
            "[Epoch 44] Loss: 8.5336 | Val Accuracy: 50.41%\n",
            "[Epoch 45] Loss: 8.5152 | Val Accuracy: 50.75%\n",
            "[Epoch 46] Loss: 8.5116 | Val Accuracy: 51.08%\n",
            "[Epoch 47] Loss: 8.4732 | Val Accuracy: 49.25%\n",
            "[Epoch 48] Loss: 8.4786 | Val Accuracy: 51.58%\n",
            "[Epoch 49] Loss: 8.4356 | Val Accuracy: 52.90%\n",
            "[Epoch 50] Loss: 8.4684 | Val Accuracy: 53.07%\n",
            "[Epoch 51] Loss: 8.4717 | Val Accuracy: 52.40%\n",
            "[Epoch 52] Loss: 8.4688 | Val Accuracy: 52.07%\n",
            "[Epoch 53] Loss: 8.4714 | Val Accuracy: 50.75%\n",
            "[Epoch 54] Loss: 8.4255 | Val Accuracy: 52.57%\n",
            "[Epoch 55] Loss: 8.4009 | Val Accuracy: 53.73%\n",
            "[Epoch 56] Loss: 8.4071 | Val Accuracy: 50.58%\n",
            "[Epoch 57] Loss: 8.4143 | Val Accuracy: 50.25%\n",
            "[Epoch 58] Loss: 8.4148 | Val Accuracy: 53.07%\n",
            "[Epoch 59] Loss: 8.4227 | Val Accuracy: 50.75%\n",
            "[Epoch 60] Loss: 8.3613 | Val Accuracy: 52.40%\n",
            "[Epoch 61] Loss: 8.3862 | Val Accuracy: 52.90%\n",
            "[Epoch 62] Loss: 8.3842 | Val Accuracy: 53.23%\n",
            "[Epoch 63] Loss: 8.3834 | Val Accuracy: 50.25%\n",
            "[Epoch 64] Loss: 8.3696 | Val Accuracy: 52.40%\n",
            "[Epoch 65] Loss: 8.3531 | Val Accuracy: 50.58%\n",
            "[Epoch 66] Loss: 8.2832 | Val Accuracy: 53.23%\n",
            "[Epoch 67] Loss: 8.2775 | Val Accuracy: 54.73%\n",
            "[Epoch 68] Loss: 8.2319 | Val Accuracy: 51.08%\n",
            "[Epoch 69] Loss: 8.3266 | Val Accuracy: 55.72%\n",
            "[Epoch 70] Loss: 8.3417 | Val Accuracy: 55.72%\n",
            "[Epoch 71] Loss: 8.2692 | Val Accuracy: 53.07%\n",
            "[Epoch 72] Loss: 8.2563 | Val Accuracy: 53.57%\n",
            "[Epoch 73] Loss: 8.1938 | Val Accuracy: 55.39%\n",
            "[Epoch 74] Loss: 8.1917 | Val Accuracy: 55.72%\n",
            "[Epoch 75] Loss: 8.2056 | Val Accuracy: 54.06%\n",
            "[Epoch 76] Loss: 8.2261 | Val Accuracy: 53.40%\n",
            "[Epoch 77] Loss: 8.1914 | Val Accuracy: 55.22%\n",
            "[Epoch 78] Loss: 8.1829 | Val Accuracy: 57.05%\n",
            "[Epoch 79] Loss: 8.1119 | Val Accuracy: 57.05%\n",
            "[Epoch 80] Loss: 8.1348 | Val Accuracy: 57.05%\n",
            "[Epoch 81] Loss: 8.2284 | Val Accuracy: 53.73%\n",
            "[Epoch 82] Loss: 8.1667 | Val Accuracy: 53.73%\n",
            "[Epoch 83] Loss: 8.2140 | Val Accuracy: 54.89%\n",
            "[Epoch 84] Loss: 8.1777 | Val Accuracy: 56.05%\n",
            "[Epoch 85] Loss: 8.1656 | Val Accuracy: 55.89%\n",
            "[Epoch 86] Loss: 8.0056 | Val Accuracy: 55.22%\n",
            "[Epoch 87] Loss: 8.0502 | Val Accuracy: 57.38%\n",
            "[Epoch 88] Loss: 7.9715 | Val Accuracy: 56.55%\n",
            "[Epoch 89] Loss: 8.0145 | Val Accuracy: 54.73%\n",
            "[Epoch 90] Loss: 7.9882 | Val Accuracy: 52.90%\n",
            "[Epoch 91] Loss: 7.9951 | Val Accuracy: 57.38%\n",
            "[Epoch 92] Loss: 7.9694 | Val Accuracy: 53.90%\n",
            "[Epoch 93] Loss: 8.0319 | Val Accuracy: 56.55%\n",
            "[Epoch 94] Loss: 8.0070 | Val Accuracy: 57.05%\n",
            "[Epoch 95] Loss: 8.0076 | Val Accuracy: 54.73%\n",
            "[Epoch 96] Loss: 7.9198 | Val Accuracy: 54.06%\n",
            "[Epoch 97] Loss: 8.0250 | Val Accuracy: 54.89%\n",
            "[Epoch 98] Loss: 8.1032 | Val Accuracy: 57.21%\n",
            "[Epoch 99] Loss: 7.9602 | Val Accuracy: 57.21%\n",
            "[Epoch 100] Loss: 7.9313 | Val Accuracy: 56.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = evaluate(test_loader)\n",
        "print(f\"Test Accuracy: {test_acc:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMkd_0uxqNaw",
        "outputId": "d500d46c-561d-4a71-ebe5-5aef7558d51c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 53.78%\n"
          ]
        }
      ]
    }
  ]
}