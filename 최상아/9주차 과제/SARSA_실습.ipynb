{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3d158d",
   "metadata": {},
   "source": [
    "# [강화 시스터즈] SARSA 실습\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1aVIHjGqCYCI_Y6x6ym4PZiDtwGUOTdVx\n",
    "\n",
    "## SARSA\n",
    "다음 환경(**GridWorldEnvironment**)을 가지고 SARSA를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c2006",
   "metadata": {},
   "source": [
    "## GridWorldEnvironment\n",
    "\n",
    "```py\n",
    "start_point = (0,0)\n",
    "end_point = (4,4)\n",
    "gridworld_size = (5,5)\n",
    "env = GridWorldEnvironment(start_point, end_point, grid_world_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7fbf0",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, start_point:Tuple, end_point:Tuple, grid_world_size:Tuple):\n",
    "        # 시작점과 끝점을 받는다.\n",
    "        self.start_point = start_point\n",
    "        self.end_point = end_point if end_point != (-1,-1) else (grid_world_size[0] + end_point[0],\n",
    "                                                                 grid_world_size[1] + end_point[1])\n",
    "\n",
    "        # 그리드 월드의 규격을 받는다.\n",
    "        self.width, self.height = grid_world_size\n",
    "\n",
    "        # action dictionary\n",
    "        self.action_space = ['up', 'down', 'left', 'right']\n",
    "        self.num_actions = len(self.action_space)\n",
    "        self.actions = {'up':(-1,0),\n",
    "                        'down':(1,0),\n",
    "                        'left':(0,-1),\n",
    "                        'right':(0,1) }\n",
    "\n",
    "        # 상태 : 좌표로 나타남\n",
    "        self.traces = []\n",
    "\n",
    "        # total states\n",
    "        self.total_states = []\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                self.total_states.append((x,y))\n",
    "\n",
    "        # reward\n",
    "        self.reward = np.zeros(shape=(self.height, self.width)).tolist()\n",
    "        self.reward[end_point[0]][end_point[1]] = 1\n",
    "\n",
    "    def render(self):\n",
    "        # 그리드 월드의 상태를 출력한다.\n",
    "        self.grid_world = np.full(shape=(self.height, self.width), fill_value=\".\").tolist()\n",
    "\n",
    "        last_point = self.traces[-1] # 에이전트가 가장 마지막에 있었던 위치\n",
    "\n",
    "        traces = list(set(self.traces)) # 중복된 값을 삭제하기 위함\n",
    "        for trace in traces:\n",
    "            self.grid_world[trace[0]][trace[1]] = \"X\"\n",
    "\n",
    "        self.grid_world[self.start_point[0]][self.start_point[1]] = \"S\" # start point\n",
    "        self.grid_world[self.end_point[0]][self.end_point[1]] = \"G\" # end point\n",
    "        self.grid_world[last_point[0]][last_point[1]] = \"A\" # 현재 에이전트의 위치\n",
    "\n",
    "        # string으로 출력한다.\n",
    "        grid = \"\"\n",
    "\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                grid += self.grid_world[i][j]+\" \"\n",
    "            grid += \"\\n\"\n",
    "\n",
    "        print(grid)\n",
    "\n",
    "    def get_reward(self, state, action_idx):\n",
    "        next_state = self.state_after_action(state, action_idx)\n",
    "        return self.reward[next_state[0]][next_state[1]]\n",
    "\n",
    "    def state_after_action(self, state, action_idx:int):\n",
    "        action = self.action_space[action_idx]\n",
    "        row_movement, col_movement = self.actions[action]\n",
    "\n",
    "        # action에 따라 에이전트 이동\n",
    "        next_state = (state[0]+row_movement, state[1]+col_movement)\n",
    "        next_state = self.check_boundary(next_state)\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def check_boundary(self, state):\n",
    "        state = list(state)\n",
    "        state[0] = (0 if state[0] < 0 else self.height - 1 if state[0] > self.height - 1 else state[0])\n",
    "        state[1] = (0 if state[1] < 0 else self.width - 1 if state[1] > self.width - 1 else state[1])\n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ee77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnvironment(start_point=(0,0), #위 환경 클래스를 상속 받아 `env.render` 코드를 구현\n",
    "                           end_point=(4,4),\n",
    "                           grid_world_size=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc90c78",
   "metadata": {},
   "source": [
    "# SARSA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MySARSAEnv(GridWorldEnvironment):\n",
    "  def __init__(self, start_point, end_point, grid_world_size, alpha=0.1, gamma=0.1, epsilon=0.1):\n",
    "    super().__init__(start_point, end_point, grid_world_size)\n",
    "    self.env = self\n",
    "    self.alpha = alpha # learning rate\n",
    "    self.gamma = gamma # discount (낮을수록 현재 보상을, 1에 가까울수록 미래 보상을 중요하게 여김)\n",
    "    self.epsilon = epsilon # exploration rate (최적 기대값 외 다른 행동도 일정 확률로 선택, 탐험)\n",
    "    # Q 테이블 초기화 (상태-행동)\n",
    "    self.Qtable = {}\n",
    "    for state in self.total_states:\n",
    "      for a in range(len(self.action_space)):\n",
    "        self.Qtable[(state,a)] = 0.0\n",
    "\n",
    "  def choose_action(self, state): # 행동 선택 함수\n",
    "    if np.random.rand() < self.epsilon: # 무작위(0~1) 수가 epsilon보다 작으면 탐험, 크면 활용\n",
    "      return np.random.choice(range(len(self.action_space))) # 행동 랜덤하게 선택\n",
    "    else:\n",
    "      qs = [self.Qtable[(state,a)] for a in range(len(self.action_space))]\n",
    "      return np.argmax(qs) # 최대 기대값의 행동 반환\n",
    "\n",
    "  def learn(self, state, action, reward, next_state, next_action):\n",
    "    # 0값 업데이트 (SARSA)\n",
    "    current_q = self.Qtable[(state, action)]\n",
    "    next_state_q = self.Qtable[(next_state, next_action)]\n",
    "    self.Qtable[(state, action)] += self.alpha * (reward + self.gamma * next_state_q - current_q)\n",
    "\n",
    "  def run_sarsa(self, episode = 20):\n",
    "    for ep in range(episode):\n",
    "      state = self.start_point\n",
    "      self.env.traces = [state]\n",
    "      action = self.choose_action(state)\n",
    "\n",
    "      while True:\n",
    "        reward = self.get_reward(state, action)\n",
    "        next_state = self.state_after_action(state, action)\n",
    "        next_action = self.choose_action(next_state)\n",
    "        self.env.traces.append(next_state)\n",
    "\n",
    "        self.learn(state, action, reward, next_state, next_action)\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "        if next_state == self.end_point:\n",
    "          break\n",
    "\n",
    "      print(f\"Episode: {ep+1} complete\")\n",
    "      self.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581706f",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "- **SARSA**를 이용해 그리드 월드 학습시키기  \n",
    "- 학습 지표 시각화 (에피소드마다 에이전트의 이동 횟수 시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b459a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = MySARSAEnv(start_point=(0,0),\n",
    "                  end_point=(4,4),\n",
    "                  grid_world_size=(5,5))\n",
    "\n",
    "env1.run_sarsa()\n",
    "\n",
    "env2 = MySARSAEnv(start_point=(0,0),\n",
    "                  end_point=(4,4),\n",
    "                  grid_world_size=(5,5))\n",
    "\n",
    "env2.run_sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f836422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_iterations = 20\n",
    "q_diff_list = []  # 기대값 또는 Q값 차이 기록\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    old_Q_table = copy.deepcopy(env2.Qtable)  # 이전 Q값 저장\n",
    "\n",
    "    env2.run_sarsa(1)  # 1회 반복마다 기대값 업데이트 수행\n",
    "\n",
    "    # Q값 차이 최대값 계산\n",
    "    max_diff = 0\n",
    "    for key in env2.Qtable:\n",
    "        s, a = key\n",
    "        q_val = env2.Qtable[key]\n",
    "        old_q_val = old_Q_table.get(key, 0)\n",
    "        diff = abs(q_val - old_q_val)\n",
    "        max_diff = max(max_diff, diff)\n",
    "    q_diff_list.append(max_diff)\n",
    "\n",
    "    if max_diff < 1e-4:\n",
    "        print(\"Q값 수렴 완료\")\n",
    "        break\n",
    "\n",
    "# 기대값 차이 그래프\n",
    "plt.plot(q_diff_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Max Q Value Difference\")\n",
    "plt.title(\"Q Value Convergence over SARSA\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
